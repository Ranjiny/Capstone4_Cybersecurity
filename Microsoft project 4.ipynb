{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV,cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import gc\n",
    "# Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Data Exploration and Understanding\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r'D:\\ranjiny\\Guvi_python\\capstone_microsoft\\GUIDE_Train.csv')\n",
    "\n",
    "# Step 1 a: Initial inspection\n",
    "print(df.info())\n",
    "print(df.describe(include='all'))\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# Distribution of the target variable\n",
    "sns.countplot(x='IncidentGrade', data=df)\n",
    "plt.title('Distribution of IncidentGrade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 b: Exploratory Data Analysis (EDA):\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['Category','ActionGrouped','ActionGranular','EntityType', 'EvidenceRole', \n",
    "        'ResourceType', 'Roles','OSFamily', 'OSVersion','AntispamDirection','SuspicionLevel','LastVerdict']\n",
    "for col in columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(data=df, x=col, hue='IncidentGrade')\n",
    "    plt.title(f\"{col} vs Incident grade count\")\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate and align ticks\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "    plt.legend(title='Incident Grade' )\n",
    "    plt.show()\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crosstab and Chi2 test\n",
    "\n",
    "#Null Hypothesis : the cols are independent with Incident grade (not correlated)\n",
    "#Alternate Hypothesis : the cols are correlated\n",
    "#if p_value is >=0.05 then failed to reject null hypothesis\n",
    "\n",
    "\n",
    "# Initialize summary list\n",
    "summary_results = []\n",
    "\n",
    "# Loop through columns\n",
    "for col in df.columns:\n",
    "    if col != 'IncidentGrade':  # Exclude the target column\n",
    "        crosstab = pd.crosstab(df[col], df['IncidentGrade'])\n",
    "        print(f\"Crosstab of {col} and IncidentGrade:\\n{crosstab}\\n\")\n",
    "        \n",
    "        # Perform Chi-Squared Test\n",
    "        chi2, p, dof, expected = chi2_contingency(crosstab)\n",
    "        print(f\"Chi-squared: {chi2}, p-value: {p}\\n\")\n",
    "        \n",
    "\n",
    "        summary_results.append({\n",
    "            'Column': col,\n",
    "            'Chi2 Statistic': chi2,\n",
    "            'p-value': p,\n",
    "            'Degrees of Freedom': dof,\n",
    "            'Significant': p < 0.05\n",
    "        })\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary_df = pd.DataFrame(summary_results)\n",
    "\n",
    "\n",
    "print(summary_df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2:Data Preprocessing\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of null values for each column\n",
    "null_percentage = df.isnull().sum() / len(df) * 100\n",
    "\n",
    "# # Create a DataFrame for better readability\n",
    "null_percentage_df = pd.DataFrame(null_percentage).reset_index()\n",
    "null_percentage_df.columns = ['Column', 'Percentage of Nulls']\n",
    "\n",
    "print(null_percentage_df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2a:Handling Missing Data\n",
    "#drop columns with null values > 50%\n",
    "df.drop(columns=['MitreTechniques','ActionGrouped','ActionGranular','EmailClusterId','ThreatFamily','ResourceType','Roles','SuspicionLevel'\n",
    "                 ,'AntispamDirection','LastVerdict'],axis=1,inplace=True)\n",
    "\n",
    "#drop null values for df.dropna(inplace=True)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2b:Feature Engineering\n",
    "\n",
    "# Convert timestamps to datetime\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "# Extract features from the timestamp and drop it\n",
    "#df['Hour'] = df['Timestamp'].dt.hour\n",
    "df['Day_of_week'] = df['Timestamp'].dt.dayofweek.astype('int8')\n",
    "\n",
    "df.drop(columns=['Timestamp'],axis=1,inplace=True)\n",
    "print(df.info())\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Hour','Day_of_week']:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(data=df, x=col, hue='IncidentGrade')\n",
    "    plt.title(f\"{col} vs Incident grade count\")\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate and align ticks\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "    plt.legend(title='Incident Grade')\n",
    "    plt.show()\n",
    "gc.collect()\n",
    "\n",
    "for col in ['Hour','Day_of_week']:\n",
    "    crosstab = pd.crosstab(df[col], df['IncidentGrade'])\n",
    "    print(f\"Crosstab of {col} and IncidentGrade : {crosstab}\" )\n",
    "    # Perform Chi-Squared Test\n",
    "    chi2, p, dof, expected = chi2_contingency(crosstab)\n",
    "    print(f\"Chi-squared: {chi2}, p-value: {p}\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2c:Encoding Categorical Variables: \n",
    "gc.collect()\n",
    "def Top_3_Cate(A):\n",
    "    print(A)\n",
    "    top_3_categories = df[A].value_counts().nlargest(3).index\n",
    "\n",
    "    # Step 2: Replace other categories with 'rest'\n",
    "    df[A] = df[A].apply(lambda x: x if x in top_3_categories else 'Others')\n",
    "    return A\n",
    "\n",
    "for col in ['Category', 'EntityType','EvidenceRole','OSFamily','OSVersion','CountryCode']:\n",
    "    Top_3_Cate(col)\n",
    "\n",
    "# One-Hot Encoding\n",
    "df = pd.get_dummies(df, columns=['Category', 'EntityType','EvidenceRole','OSFamily','OSVersion','CountryCode'],dtype='int8',drop_first=True)\n",
    "\n",
    "gc.collect()\n",
    "# Label encoding\n",
    "model = LabelEncoder()\n",
    "\n",
    "for col in (['Id','OrgId','IncidentId','AlertId','DetectorId','AlertTitle','DeviceId','Sha256','IpAddress','Url','AccountSid'\n",
    "             ,'AccountUpn','AccountObjectId','AccountName','DeviceName','NetworkMessageId','RegistryKey','RegistryValueName','RegistryValueData'\n",
    "            ,'OAuthApplicationId','ApplicationId','ApplicationName','FileName','FolderPath','ResourceIdName','State','City','IncidentGrade']):\n",
    "    df[col] = model.fit_transform(df[col])\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3:Data Splitting \n",
    "gc.collect()\n",
    "X = df.drop(columns=['IncidentGrade'],axis=1)  # Drop output column \n",
    "y = df['IncidentGrade']\n",
    "\n",
    "#Step 3a: Train-Validation Split & Step 3b: Stratification\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=.3,stratify=y, random_state=42)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_metrics(y_input,y_pred):\n",
    "    #print(\"*******Train******\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_input,y_pred)}\")\n",
    "    print(f\"Precision: {precision_score(y_input,y_pred,average='macro')}\")\n",
    "    print(f\"Recall : {recall_score(y_input,y_pred,average='macro')}\")\n",
    "    print(f\"F1 Score: {f1_score(y_input,y_pred,average='macro')}\")\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_input, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1', 'Class 2'])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix for train\")\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[LogisticRegression(),DecisionTreeClassifier(),RandomForestClassifier(),XGBClassifier(),LGBMClassifier()]\n",
    "\n",
    "for model in models:\n",
    "    print(type(model).__name__)\n",
    "    model.fit(x_train,y_train)\n",
    "\n",
    "    cvs=cross_val_score(model,X,y,cv=5)\n",
    "    print(f\"Cross validation score:{cvs}\")\n",
    "    \n",
    "    train_pred = model.predict(x_train)\n",
    "    test_pred = model.predict(x_test)\n",
    "    print(\"*******Train******\")\n",
    "    model_metrics(y_train,train_pred)\n",
    "    print(\"*******Test******\")\n",
    "    model_metrics(y_test,test_pred)\n",
    "gc.collect()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural network model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# # Convert to PyTorch tensors\n",
    "x_train, x_test = torch.tensor(x_train.values, dtype=torch.float32), torch.tensor(x_test.values, dtype=torch.float32)\n",
    "y_train, y_test = torch.tensor(y_train.values, dtype=torch.long), torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader for batch processing\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "# Create DataLoader instances for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)  # Use batch_size = 64 for training\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)   # Use batch_size = 64 for testing\n",
    "\n",
    "# Define Neural Network\n",
    "class CyberNN(nn.Module):\n",
    "    def __init__(self, size_nn, nclass):\n",
    "        super(CyberNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(size_nn, 175)\n",
    "        self.fc2 = nn.Linear(175, 50)\n",
    "        self.fc3 = nn.Linear(50, nclass)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = torch.relu(self.fc1(X))\n",
    "        X = torch.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "        return X\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model = CyberNN(len(X.columns), len(y.unique()))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    all_train_preds = []\n",
    "    all_train_labels = []\n",
    "    \n",
    "    # Loop over the training data in batches\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        output = model(inputs)  # Forward pass\n",
    "        loss = criterion(output, labels)  # Calculate loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update the weights\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Collect predictions and labels for metrics calculation\n",
    "        _, preds = torch.max(output, 1)\n",
    "        all_train_preds.append(preds)\n",
    "        all_train_labels.append(labels)\n",
    "        \n",
    "    # if epoch % 10 ==0 or epoch == epochs:\n",
    "    # # Calculate the average loss for the epoch\n",
    "    print(f\"{epoch}/{epochs} Loss: {running_loss / len(train_loader):.5f}\")\n",
    "\n",
    "# Evaluate the Model on Training Data\n",
    "with torch.no_grad():\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Get predictions and labels for training data\n",
    "    all_train_preds = torch.cat(all_train_preds)\n",
    "    all_train_labels = torch.cat(all_train_labels)\n",
    "    \n",
    "       \n",
    "    print(\"*******Train******\")\n",
    "    model_metrics(all_train_labels, all_train_preds)\n",
    "\n",
    "# Evaluate the Model on Testing Data\n",
    "with torch.no_grad():\n",
    "    all_test_preds = []\n",
    "    all_test_labels = []\n",
    "    \n",
    "    # Loop over the test data in batches\n",
    "    for inputs, labels in test_loader:\n",
    "        output = model(inputs)  # Forward pass\n",
    "        _, preds = torch.max(output, 1)\n",
    "        \n",
    "        all_test_preds.append(preds)\n",
    "        all_test_labels.append(labels)\n",
    "    \n",
    "    # Concatenate all test predictions and labels\n",
    "    all_test_preds = torch.cat(all_test_preds)\n",
    "    all_test_labels = torch.cat(all_test_labels)\n",
    "    \n",
    "\n",
    "    print(\"*******Test******\")\n",
    "    model_metrics(all_test_labels,all_test_preds)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
